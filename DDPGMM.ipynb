{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow_probability.python as tfpy\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfm = tf.math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "import sklearn as sk\n",
    "import statsmodels as stat\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from collections import namedtuple, Counter\n",
    "import copy\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "import json\n",
    "import alpaca_trade_api as tradeapi\n",
    "from alpaca_trade_api.rest import REST\n",
    "from datetime import timedelta\n",
    "from pytz import timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKER='SPY'\n",
    "DATE='2021-02-03'\n",
    "df_trades=pd.read_csv(\"./trades/\"+TICKER+\"/\"+TICKER+\"_trades_\"+DATE+\".csv\")\n",
    "df_trades['time']=pd.to_datetime(df_trades['time'])\n",
    "diff_size=np.array(df_trades['size'].diff())\n",
    "diff_time=np.array(df_trades['time'].diff())\n",
    "log_price=np.array(np.log(df_trades['price']))\n",
    "diff_time=np.array([np.nan]+[np.float64(i) for i in diff_time[1:]])\n",
    "diff_price=np.array(df_trades['price'].diff())\n",
    "logdiff_price=np.array(np.log(df_trades['price'])-np.log(df_trades['price'].shift(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Z_SCORE(S):\n",
    "    return (S-S.mean())/S.std()\n",
    "\n",
    "def VWAP(group,avg_column,weight_column):\n",
    "    p=group[avg_column]\n",
    "    v=group[weight_column]\n",
    "    return (p*v).sum()/v.sum()\n",
    "\n",
    "def VOLUME(group,size_column):\n",
    "    size=group[size_column]\n",
    "    return size.sum()\n",
    "\n",
    "def OHLC(group,price_column):\n",
    "    open_price=group[price_column][0]\n",
    "    high_price=group[price_column].max()\n",
    "    low_price=group[price_column].min()\n",
    "    close_price=group[price_column][-1]\n",
    "    return open_price,high_price,low_price,close_price\n",
    "\n",
    "def MEAN(group,price_column):\n",
    "    price=group[price_column]\n",
    "    return price.mean()\n",
    "\n",
    "def STD(group,price_column):\n",
    "    price=group[price_column]\n",
    "    return price.std()\n",
    "\n",
    "def SKEW(group,price_column):\n",
    "    price=group[price_column]\n",
    "    return price.skew()\n",
    "\n",
    "def KURT(group,price_column):\n",
    "    price=group[price_column]\n",
    "    return price.kurt()\n",
    "\n",
    "def LOG_RETURN(group,logdiff_price_column):\n",
    "    logdiff=group[logdiff_price_column]\n",
    "    return logdiff.sum()\n",
    "\n",
    "def REALIZED_VOLATILITY(group,logdiff_price_column):\n",
    "    logdiff_2=(group[logdiff_price_column]**2).sum()\n",
    "    return np.sqrt(logdiff_2)\n",
    "    \n",
    "def CASH_VOLUME(group,long_cash_column):\n",
    "    cash_vlume=group[long_cash_column]\n",
    "    return cash_vlume.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_groups(grouped_object):\n",
    "    for name,group in grouped_object:\n",
    "        print('\\n',name)\n",
    "        print(group.head(5).append(group.tail(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df={\n",
    "    'timestamp':np.array(df_trades['time']),\n",
    "    'diff_time':diff_time,\n",
    "    'price':np.array(df_trades['price']),\n",
    "    'log_price':log_price,\n",
    "    'diff_price':diff_price,\n",
    "    'diff_price_2':diff_price**2,\n",
    "    'logdiff_price':logdiff_price,\n",
    "    'logdiff_price_2':logdiff_price**2,\n",
    "    'size_long':np.array(df_trades['size']),\n",
    "    'long_cash':np.array(df_trades['size'])*np.array(df_trades['price']),\n",
    "    'size_short':-np.array(df_trades['size']),\n",
    "    'short_cash':-np.array(df_trades['size'])*np.array(df_trades['price']),\n",
    "    'diff_size':diff_size,\n",
    "    'diff_size_2':diff_size**2\n",
    "}\n",
    "df=pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equal time interval resampling\n",
    "freq='1min'\n",
    "time_grouped=df.set_index('timestamp').groupby(pd.Grouper(freq=freq,label='right',closed='left'))\n",
    "idx=np.cumsum(time_grouped.size())-1\n",
    "t_time=df['timestamp'][idx].values\n",
    "\n",
    "data_time_vwap=time_grouped.apply(VWAP,'price','size_long')\n",
    "\n",
    "data_time_vol=time_grouped.apply(VOLUME,'size_long')\n",
    "data_time_cashvol=time_grouped.apply(CASH_VOLUME,'long_cash')\n",
    "\n",
    "data_time_logreturn=time_grouped.apply(LOG_RETURN,'logdiff_price')\n",
    "data_time_rv=time_grouped.apply(REALIZED_VOLATILITY,'logdiff_price')\n",
    "\n",
    "data_time_mean=time_grouped.apply(MEAN,'price')\n",
    "data_time_std=time_grouped.apply(STD,'price')\n",
    "data_time_skew=time_grouped.apply(SKEW,'diff_price')\n",
    "data_time_kurt=time_grouped.apply(KURT,'diff_price')\n",
    "\n",
    "data_time_dtmean=time_grouped.apply(MEAN,'diff_time')\n",
    "data_time_dtstd=time_grouped.apply(STD,'diff_time')\n",
    "data_time_dtskew=time_grouped.apply(SKEW,'diff_time')\n",
    "data_time_dtkurt=time_grouped.apply(KURT,'diff_time')\n",
    "\n",
    "data_time_volmean=time_grouped.apply(MEAN,'size_long')\n",
    "data_time_volstd=time_grouped.apply(STD,'size_long')\n",
    "data_time_volskew=time_grouped.apply(SKEW,'size_long')\n",
    "data_time_volkurt=time_grouped.apply(KURT,'size_long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equal tick number resampling\n",
    "num_time_bars=len(time_grouped)\n",
    "total_ticks=len(df)\n",
    "num_ticks_per_bar=total_ticks/num_time_bars\n",
    "num_ticks_per_bar=int(round(num_ticks_per_bar,-2))\n",
    "tick_grouped=df.assign(groupID=lambda df : np.arange(len(df))//num_ticks_per_bar).groupby('groupID')\n",
    "idx=np.cumsum(tick_grouped.size())-1\n",
    "t_tick=df['timestamp'][idx].values\n",
    "\n",
    "data_tick_vwap=tick_grouped.apply(VWAP,'price','size_long')\n",
    "\n",
    "data_tick_vol=tick_grouped.apply(VOLUME,'size_long')\n",
    "data_tick_cashvol=tick_grouped.apply(CASH_VOLUME,'long_cash')\n",
    "\n",
    "data_tick_logreturn=tick_grouped.apply(LOG_RETURN,'logdiff_price')\n",
    "data_tick_rv=tick_grouped.apply(REALIZED_VOLATILITY,'logdiff_price')\n",
    "\n",
    "data_tick_mean=tick_grouped.apply(MEAN,'price')\n",
    "data_tick_std=tick_grouped.apply(STD,'price')\n",
    "data_tick_skew=tick_grouped.apply(SKEW,'diff_price')\n",
    "data_tick_kurt=tick_grouped.apply(KURT,'diff_price')\n",
    "\n",
    "data_tick_dtmean=tick_grouped.apply(MEAN,'diff_time')\n",
    "data_tick_dtstd=tick_grouped.apply(STD,'diff_time')\n",
    "data_tick_dtskew=tick_grouped.apply(SKEW,'diff_time')\n",
    "data_tick_dtkurt=tick_grouped.apply(KURT,'diff_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equal volume resampling\n",
    "data_cum_volume=df.assign(cumVolume=df['size_long'].cumsum())\n",
    "total_volume=data_cum_volume.cumVolume.values[-1]\n",
    "volume_per_bar=total_volume/num_time_bars\n",
    "volume_per_bar=int(round(volume_per_bar,-2))\n",
    "volume_grouped=data_cum_volume.assign(groupID=lambda df: df.cumVolume//volume_per_bar).groupby('groupID')\n",
    "\n",
    "idx=np.cumsum(volume_grouped.size())-1\n",
    "t_volume=df['timestamp'][idx].values\n",
    "\n",
    "data_vol_vwap=volume_grouped.apply(VWAP,'price','size_long')\n",
    "\n",
    "data_vol_vol=volume_grouped.apply(VOLUME,'size_long')\n",
    "data_vol_cashvol=volume_grouped.apply(CASH_VOLUME,'long_cash')\n",
    "\n",
    "data_vol_logreturn=volume_grouped.apply(LOG_RETURN,'logdiff_price')\n",
    "data_vol_rv=volume_grouped.apply(REALIZED_VOLATILITY,'logdiff_price')\n",
    "\n",
    "data_vol_mean=volume_grouped.apply(MEAN,'price')\n",
    "data_vol_std=volume_grouped.apply(STD,'price')\n",
    "data_vol_skew=volume_grouped.apply(SKEW,'diff_price')\n",
    "data_vol_kurt=volume_grouped.apply(KURT,'diff_price')\n",
    "\n",
    "data_vol_dtmean=volume_grouped.apply(MEAN,'diff_time')\n",
    "data_vol_dtstd=volume_grouped.apply(STD,'diff_time')\n",
    "data_vol_dtskew=volume_grouped.apply(SKEW,'diff_time')\n",
    "data_vol_dtkurt=volume_grouped.apply(KURT,'diff_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equal cash volume resampling\n",
    "data_cum_cashvol=df.assign(cumCashVol=df['long_cash'].cumsum())\n",
    "total_cum_cashvol=data_cum_cashvol.cumCashVol.values[-1]\n",
    "cashvol_per_bar=total_cum_cashvol/num_time_bars\n",
    "cashvol_per_bar=int(round(cashvol_per_bar,-2))\n",
    "cashvol_grouped=data_cum_cashvol.assign(groupID=lambda df: df.cumCashVol//cashvol_per_bar).groupby('groupID')\n",
    "\n",
    "idx=np.cumsum(cashvol_grouped.size())-1\n",
    "t_cashvol=df['timestamp'][idx].values\n",
    "\n",
    "data_cashvol_vwap=cashvol_grouped.apply(VWAP,'price','size_long')\n",
    "\n",
    "data_cashvol_vol=cashvol_grouped.apply(VOLUME,'size_long')\n",
    "data_cashvol_cashvol=cashvol_grouped.apply(CASH_VOLUME,'long_cash')\n",
    "\n",
    "data_cashvol_logreturn=cashvol_grouped.apply(LOG_RETURN,'logdiff_price')\n",
    "data_cashvol_rv=cashvol_grouped.apply(REALIZED_VOLATILITY,'logdiff_price')\n",
    "\n",
    "data_cashvol_mean=cashvol_grouped.apply(MEAN,'price')\n",
    "data_cashvol_std=cashvol_grouped.apply(STD,'price')\n",
    "data_cashvol_skew=cashvol_grouped.apply(SKEW,'diff_price')\n",
    "data_cashvol_kurt=cashvol_grouped.apply(KURT,'diff_price')\n",
    "\n",
    "data_cashvol_dtmean=cashvol_grouped.apply(MEAN,'diff_time')\n",
    "data_cashvol_dtstd=cashvol_grouped.apply(STD,'diff_time')\n",
    "data_cashvol_dtskew=cashvol_grouped.apply(SKEW,'diff_time')\n",
    "data_cashvol_dtkurt=cashvol_grouped.apply(KURT,'diff_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dirichlet process\n",
    "def getTHETA(base_measure,sample_num):\n",
    "    theta=base_measure.sample(sample_num)\n",
    "    return theta\n",
    "\n",
    "def getV(alpha,sample_num):\n",
    "    V=tfd.Beta(concentration1=1.,concentration0=alpha).sample(sample_num)\n",
    "    return V\n",
    "\n",
    "def getClusterInfo(sample_num,logits=None,probs=None):\n",
    "    elem_in_which_cluster=tfd.Categorical(logits=logits,probs=probs).sample(sample_num)\n",
    "    unique_cluster_index,elem_to_unique_cluster_array_index,elem_num_in_unique_cluster=tf.unique_with_counts(elem_in_which_cluster)\n",
    "    return elem_in_which_cluster,unique_cluster_index,elem_to_unique_cluster_array_index,elem_num_in_unique_cluster\n",
    "\n",
    "\n",
    "alpha=10.\n",
    "#alpha=tfd.InverseGamma(concentration=np.ones([1], dtype=dtype),rate=np.ones([1]),name='rv_alpha')\n",
    "sample_num=10000\n",
    "\n",
    "\n",
    "base=tfd.Normal(loc=0.,scale=1.)\n",
    "THETA=getTHETA(base_measure=base,sample_num=sample_num)# we have the x loc\n",
    "V=getV(alpha=alpha,sample_num=sample_num)# we want to construct the pi\n",
    "PI=tfm.exp(tfm.log(V)+tfm.cumsum(tfm.log(1.-V),exclusive=True,axis=0))\n",
    "probs_PI=PI/tf.reduce_sum(PI)\n",
    "logits_PI=tfm.log(PI/(1.-PI))\n",
    "\n",
    "elem_in_which_cluster,unique_cluster_index,elem_to_unique_cluster_array_index,elem_num_in_unique_cluster=getClusterInfo(sample_num=sample_num,logits=logits_PI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussian Mixture\n",
    "mix_gauss = tfd.Mixture(\n",
    "  cat=tfd.Categorical(probs=probs_PI),\n",
    "  components=[\n",
    "    tfd.Normal(loc=THETA[i], scale=0.1) for i in range(len(THETA))])\n",
    "\n",
    "# Plot the PDF.\n",
    "x = tf.linspace(-4., 4., int(1e4))\n",
    "px.line(x=x, y=mix_gauss.prob(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##reference\n",
    "# Stickbreak function\n",
    "def stickbreak(v):\n",
    "    batch_ndims = len(v.shape) - 1\n",
    "    cumprod_one_minus_v = tf.math.cumprod(1 - v, axis=-1)\n",
    "    one_v = tf.pad(v, [[0, 0]] * batch_ndims + [[0, 1]], \"CONSTANT\",\n",
    "                   constant_values=1)\n",
    "    c_one = tf.pad(cumprod_one_minus_v, [[0, 0]] * batch_ndims + [[1, 0]],\n",
    "                   \"CONSTANT\", constant_values=1)\n",
    "    return one_v * c_one\n",
    "\n",
    "# See: https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MixtureSameFamily\n",
    "# See: https://www.tensorflow.org/probability/examples/Bayesian_Gaussian_Mixture_Model\n",
    "# Define model builder.\n",
    "def create_dp_sb_gmm(nobs, K, dtype=np.float64):\n",
    "    return tfd.JointDistributionNamed(dict(\n",
    "        # Mixture means\n",
    "        mu = tfd.Independent(\n",
    "            tfd.Normal(np.zeros(K, dtype), 3),\n",
    "            reinterpreted_batch_ndims=1\n",
    "        ),\n",
    "        # Mixture scales\n",
    "        sigma = tfd.Independent(\n",
    "            tfd.LogNormal(loc=np.full(K, - 2, dtype), scale=0.5),\n",
    "            reinterpreted_batch_ndims=1\n",
    "        ),\n",
    "        # Mixture weights (stick-breaking construction)\n",
    "        alpha = tfd.Gamma(concentration=np.float64(1.0), rate=10.0),\n",
    "        v = lambda alpha: tfd.Independent(\n",
    "            # tfd.Beta(np.ones(K - 1, dtype), alpha),\n",
    "            # NOTE: Dave Moore suggests doing this instead, to ensure \n",
    "            # that a batch dimension in alpha doesn't conflict with \n",
    "            # the other parameters.\n",
    "            tfd.Beta(np.ones(K - 1, dtype), alpha[..., tf.newaxis]),\n",
    "            reinterpreted_batch_ndims=1\n",
    "        ),\n",
    "\n",
    "        # Observations (likelihood)\n",
    "        obs = lambda mu, sigma, v: tfd.Sample(tfd.MixtureSameFamily(\n",
    "            # This will be marginalized over.\n",
    "            mixture_distribution=tfd.Categorical(probs=stickbreak(v)),\n",
    "            # mixture_distribution=tfd.Categorical(probs=v),\n",
    "            components_distribution=tfd.Normal(mu, sigma)),\n",
    "            sample_shape=nobs)\n",
    "    ))\n",
    "\n",
    "# Number of mixture components.\n",
    "ncomponents = 10\n",
    "\n",
    "# Create model.\n",
    "model = create_dp_sb_gmm(nobs=len(simdata['y']), K=ncomponents)\n",
    "\n",
    "# Define log unnormalized joint posterior density.\n",
    "def target_log_prob_fn(mu, sigma, alpha, v):\n",
    "    return model.log_prob(obs=y, mu=mu, sigma=sigma, alpha=alpha, v=v)\n",
    "\n",
    "# NOTE: Read data y here ...\n",
    "# Here, y (a vector of length 500) is noisy univariate draws from a\n",
    "# mixture distribution with 4 components.\n",
    "\n",
    "### ADVI ###\n",
    "# Prep work for ADVI. Credit: Thanks to Dave Moore at BayesFlow for helping\n",
    "# with the implementation!\n",
    "\n",
    "# ADVI is quite sensitive to initial distritbution.\n",
    "tf.random.set_seed(7) # 7\n",
    "\n",
    "# Create variational parameters.\n",
    "qmu_loc = tf.Variable(tf.random.normal([ncomponents], dtype=np.float64) * 3,\n",
    "                      name='qmu_loc')\n",
    "qmu_rho = tf.Variable(tf.random.normal([ncomponents], dtype=np.float64) * 2,\n",
    "                      name='qmu_rho')\n",
    "\n",
    "qsigma_loc = tf.Variable(tf.random.normal([ncomponents], dtype=np.float64) - 2,\n",
    "                         name='qsigma_loc')\n",
    "qsigma_rho = tf.Variable(tf.random.normal([ncomponents], dtype=np.float64) - 2,\n",
    "                         name='qsigma_rho')\n",
    "\n",
    "qv_loc = tf.Variable(tf.random.normal([ncomponents - 1], dtype=np.float64) - 2,\n",
    "                     name='qv_loc')\n",
    "qv_rho = tf.Variable(tf.random.normal([ncomponents - 1], dtype=np.float64) - 1,\n",
    "                     name='qv_rho')\n",
    "\n",
    "qalpha_loc = tf.Variable(tf.random.normal([], dtype=np.float64),\n",
    "                         name='qalpha_loc')\n",
    "qalpha_rho = tf.Variable(tf.random.normal([], dtype=np.float64),\n",
    "                         name='qalpha_rho')\n",
    "\n",
    "# Create variational distribution.\n",
    "surrogate_posterior = tfd.JointDistributionNamed(dict(\n",
    "    # qmu\n",
    "    mu=tfd.Independent(tfd.Normal(qmu_loc, tf.nn.softplus(qmu_rho)),\n",
    "                       reinterpreted_batch_ndims=1),\n",
    "    # qsigma\n",
    "    sigma=tfd.Independent(tfd.LogNormal(qsigma_loc,\n",
    "                                        tf.nn.softplus(qsigma_rho)),\n",
    "                          reinterpreted_batch_ndims=1),\n",
    "    # qv\n",
    "    v=tfd.Independent(tfd.LogitNormal(qv_loc, tf.nn.softplus(qv_rho)),\n",
    "                      reinterpreted_batch_ndims=1),\n",
    "    # qalpha\n",
    "    alpha=tfd.LogNormal(qalpha_loc, tf.nn.softplus(qalpha_rho))))\n",
    "\n",
    "# Run ADVI.\n",
    "losses = tfp.vi.fit_surrogate_posterior(\n",
    "    target_log_prob_fn=target_log_prob_fn,\n",
    "    surrogate_posterior=surrogate_posterior,\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=1e-2),\n",
    "    sample_size=100, seed=1, num_steps=2000)  # 9 seconds\n",
    "\n",
    "\n",
    "### MCMC (HMC/NUTS) ###\n",
    "\n",
    "# Creates initial values for HMC, NUTS. \n",
    "def generate_initial_state(seed=None):\n",
    "    tf.random.set_seed(seed)\n",
    "    return [\n",
    "        tf.zeros(ncomponents, dtype, name='mu'),\n",
    "        tf.ones(ncomponents, dtype, name='sigma') * 0.1,\n",
    "        tf.ones([], dtype, name='alpha') * 0.5,\n",
    "        tf.fill(ncomponents - 1, value=np.float64(0.5), name='v')\n",
    "    ]\n",
    "\n",
    "# Create bijectors to transform unconstrained to and from constrained\n",
    "# parameters-space.  For example, if X ~ Exponential(theta), then X is\n",
    "# constrained to be positive. A transformation that puts X onto an\n",
    "# unconstrained # space is Y = log(X). In that case, the bijector used should\n",
    "# be the **inverse-transform**, which is exp(.) (i.e. so that X = exp(Y)).\n",
    "#\n",
    "# NOTE: Define the inverse-transforms for each parameter in sequence.\n",
    "bijectors = [\n",
    "    tfb.Identity(),  # mu\n",
    "    tfb.Exp(),  # sigma\n",
    "    tfb.Exp(),  # alpha\n",
    "    tfb.Sigmoid()  # v\n",
    "]\n",
    "\n",
    "# Define HMC sampler.\n",
    "@tf.function(autograph=False, experimental_compile=True)\n",
    "def hmc_sample(num_results, num_burnin_steps, current_state, step_size=0.01,\n",
    "               num_leapfrog_steps=100):\n",
    "    return tfp.mcmc.sample_chain(\n",
    "        num_results=num_results,\n",
    "        num_burnin_steps=num_burnin_steps,\n",
    "        current_state=current_state,\n",
    "        kernel = tfp.mcmc.SimpleStepSizeAdaptation(\n",
    "            tfp.mcmc.TransformedTransitionKernel(\n",
    "                inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "                    target_log_prob_fn=target_log_prob_fn,\n",
    "                    step_size=step_size, num_leapfrog_steps=num_leapfrog_steps, seed=1),\n",
    "                bijector=bijectors),\n",
    "            num_adaptation_steps=num_burnin_steps),\n",
    "        trace_fn = lambda _, pkr: pkr.inner_results.inner_results.is_accepted)\n",
    "\n",
    "\n",
    "# Define NUTS sampler.\n",
    "@tf.function(autograph=False, experimental_compile=True)\n",
    "def nuts_sample(num_results, num_burnin_steps, current_state, max_tree_depth=10):\n",
    "    return tfp.mcmc.sample_chain(\n",
    "        num_results=num_results,\n",
    "        num_burnin_steps=num_burnin_steps,\n",
    "        current_state=current_state,\n",
    "        kernel = tfp.mcmc.SimpleStepSizeAdaptation(\n",
    "            tfp.mcmc.TransformedTransitionKernel(\n",
    "                inner_kernel=tfp.mcmc.NoUTurnSampler(\n",
    "                     target_log_prob_fn=target_log_prob_fn,\n",
    "                     max_tree_depth=max_tree_depth, step_size=0.01, seed=1),\n",
    "                bijector=bijectors),\n",
    "            num_adaptation_steps=num_burnin_steps,  # should be smaller than burn-in.\n",
    "            target_accept_prob=0.8),\n",
    "        trace_fn = lambda _, pkr: pkr.inner_results.inner_results.is_accepted)\n",
    "\n",
    "# Run HMC sampler.\n",
    "current_state = generate_initial_state()\n",
    "[mu, sigma, alpha, v], is_accepted = hmc_sample(500, 500, current_state)\n",
    "hmc_output = dict(mu=mu, sigma=sigma, alpha=alpha, v=v,\n",
    "                  acceptance_rate=is_accepted.numpy().mean())\n",
    "\n",
    "# Run NUTS sampler.\n",
    "current_state = generate_initial_state()\n",
    "[mu, sigma, alpha, v], is_accepted = nuts_sample(500, 500, current_state)\n",
    "nuts_output = dict(mu=mu, sigma=sigma, alpha=alpha, v=v,\n",
    "                   acceptance_rate=is_accepted.numpy().mean())"
   ]
  }
 ]
}